{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from functools import partial, reduce\n",
    "from typing import DefaultDict, Tuple, List, Dict\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import nlp_utils # our utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = nlp_utils.get_cache_dir()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load Llava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float16, cache_dir=cache_dir)\n",
    "model.generation_config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = nlp_utils.get_animals_data_df(limit_per_class=50)\n",
    "print(len(df_data))\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.random.randint(len(df_data), size=4)\n",
    "image_paths = [df_data.image_path.iloc[i] for i in ids]\n",
    "images = [Image.open(p) for p in image_paths]\n",
    "Image.fromarray(np.concatenate([img.resize((256, 256)) for img in images], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = Image.open(df_data.image_path[22])\n",
    "image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image2 = Image.open(df_data.image_path[66])\n",
    "image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What is common between these two images'\n",
    "question = 'Who is the CEO of google?'\n",
    "prompt = f\"[INST] <image>\\n{question} [/INST]\"\n",
    "inputs = processor(prompt, image2, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(**inputs, max_new_tokens=100, output_hidden_states=True, return_dict_in_generate=True)\n",
    "result_text = processor.decode(output['sequences'][0], skip_special_tokens=True)\n",
    "result_text = result_text.split(prompt.replace('<image>', ' '))[1].strip()\n",
    "print(f'Q: {question};\\nA: {result_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_module_names = [name for name, _ in model.named_modules()]\n",
    "# print(all_module_names[:5])\n",
    "lang_model_related_layers = [module_name for module_name in all_module_names if 'language_model.model.layers.' in module_name]\n",
    "layers = list(set([e.split('language_model.model.layers.')[1].split('.')[0] for e in lang_model_related_layers]))\n",
    "n_layers = max([int(e) for e in layers])\n",
    "n_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Save hidden states using my hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from PIL import Image\n",
    "\n",
    "def plot_model_graph(nodes, max_depth=3):\n",
    "    dot = graphviz.Digraph()\n",
    "\n",
    "    def add_edges(dot, nodes, max_depth=None):\n",
    "        edges = set()\n",
    "        for node in nodes:\n",
    "            parts = node.split('.')\n",
    "            for i in range(1, len(parts)):\n",
    "                if max_depth is not None and i >= max_depth:\n",
    "                    break\n",
    "                parent = '.'.join(parts[:i])\n",
    "                child = '.'.join(parts[:i+1])\n",
    "                edges.add((parent, child))\n",
    "        for edge in edges:\n",
    "            dot.edge(edge[0], edge[1])\n",
    "    \n",
    "    def generate_graph(nodes, max_depth=None):\n",
    "        dot = graphviz.Digraph()\n",
    "        add_edges(dot, nodes, max_depth)\n",
    "        return dot\n",
    "    \n",
    "    graph_with_depth = generate_graph(nodes, max_depth)\n",
    "    file_path_depth = \"./hierarchical_structure_graph_depth\"\n",
    "    graph_with_depth.render(file_path_depth, format='png', cleanup=True)\n",
    "    img_path = file_path_depth +'.png'\n",
    "    display(Image.open(img_path))\n",
    "\n",
    "plot_model_graph(all_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_f_string = 'language_model.model.layers.{0}'\n",
    "# layer_f_string = 'language_model.model.layers.{0}.self_attn'\n",
    "# layer_f_string = 'language_model.model.layers.{0}.self_attn.q_proj'\n",
    "# layer_f_string = 'language_model.model.layers.{0}.self_attn.k_proj'\n",
    "# layer_f_string = 'language_model.model.layers.{0}.self_attn.v_proj'\n",
    "# layer_f_string = 'language_model.model.layers.{0}.self_attn.o_proj'\n",
    "# layer_f_string = 'language_model.model.layers.{0}.self_attn.rotary_emb'\n",
    "# layer_f_string = 'language_model.model.layers.{0}.mlp'\n",
    "# layer_f_string = 'language_model.model.layers.{0}.mlp.gate_proj'\n",
    "# layer_f_string = 'language_model.model.layers.{0}.mlp.up_proj'\n",
    "# layer_f_string = 'language_model.model.layers.{0}.mlp.down_proj'\n",
    "# layer_f_string = 'language_model.model.layers.{0}.mlp.act_fn'\n",
    "# layer_f_string = 'language_model.model.layers.{0}.input_layernorm'\n",
    "layer_f_string = 'language_model.model.layers.{0}.post_attention_layernorm'\n",
    "\n",
    "print('layer_f_string:', layer_f_string, end=2*'\\n')\n",
    "\n",
    "\n",
    "layer_names = [layer_f_string.format(layer_i) for layer_i in range(n_layers)] \n",
    "layer_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_activations(\n",
    "        activations: DefaultDict,\n",
    "        name: str,\n",
    "        module: nn.Module,\n",
    "        inp: Tuple,\n",
    "        out: torch.Tensor) -> None:\n",
    "    if name == 'language_model.model.layers.2.post_attention_layernorm':\n",
    "        print('hello', len(activations[name]))\n",
    "    activations[name].append(out.detach().cpu())\n",
    "\n",
    "def get_module_by_name(module, access_string):\n",
    "    names = access_string.split(sep='.')\n",
    "    return reduce(getattr, names, module)\n",
    "\n",
    "def set_hooks(model, activations_dict, module_names_to_hook):\n",
    "    handles = []\n",
    "    for name in module_names_to_hook:\n",
    "        print('register_forward_hook to', name)\n",
    "        module = get_module_by_name(model, name)\n",
    "        hook = partial(save_activations, activations_dict, name)\n",
    "        handle = module.register_forward_hook(hook)\n",
    "        handles.append(handle)\n",
    "    return handles\n",
    "\n",
    "def remove_hooks(handles):\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "\n",
    "def get_activations(activations_dict, layer_i):\n",
    "    layer_str = layer_f_string.format(layer_i)\n",
    "    activations = activations_dict[layer_str]\n",
    "    return activations\n",
    "\n",
    "\n",
    "# activations_dict = defaultdict(list)\n",
    "# handles = set_hooks(model, activations_dict, layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_hooks(handles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get hidden states directly from the model using output_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What kind of animal is in the image? answer shortly'\n",
    "prompt = f\"[INST] <image>\\n{question} [/INST]\"\n",
    "inputs = processor(prompt, image2, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(**inputs, max_new_tokens=100, output_hidden_states=True, return_dict_in_generate=True)\n",
    "result_text = processor.decode(output['sequences'][0], skip_special_tokens=True)\n",
    "result_text = result_text.split(prompt.replace('<image>', ' '))[1].strip()\n",
    "print(f'Q: {question};\\nA: {result_text}')\n",
    "print('# input tokens:', inputs['input_ids'].shape[1])\n",
    "print('# output tokens:', output['sequences'].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image2.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['hidden_states'][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['hidden_states'][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "====================================\n",
    "     hidden_states structure\n",
    "====================================\n",
    "\n",
    "time0\n",
    "    layer_0\n",
    "        (1, 2182, 4096)  # for the first response we are getting the hidden states of all positions up to 2182\n",
    "    ....\n",
    "    layer_32\n",
    "        (1, 2182, 4096)\n",
    "time1\n",
    "    layer_0\n",
    "        (1, 1, 4096)\n",
    "    ....\n",
    "    layer_32\n",
    "        (1, 1, 4096)\n",
    "...\n",
    "timeT\n",
    "    layer_0\n",
    "        (1, 1, 4096)\n",
    "    ....\n",
    "    layer_32\n",
    "        (1, 1, 4096)   \n",
    "         \n",
    "\"\"\"\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = output['hidden_states']\n",
    "\n",
    "time = 0       # 0 < time < seuqnce_length\n",
    "layer = -1     # 0 < layer < 32\n",
    "sample = 0     # batch size is 1\n",
    "token_loc = 0  # 0 < token_loc <= time < seuqnce_length\n",
    "\n",
    "embd = hidden_states[time][layer][sample]\n",
    "embd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project hidden states on the output embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer(['water'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 0\n",
    "sample = 0\n",
    "token_loc = -1\n",
    "token_loc = -1\n",
    "\n",
    "W = model.get_output_embeddings().weight  # the Unembedding matrix (#tokens, embedding_dim) = (32000, 4096)\n",
    "\n",
    "for layer in range(28, n_layers+1):\n",
    "    print('layer -', layer)\n",
    "    embd = hidden_states[time][layer][sample][token_loc]\n",
    "\n",
    "    logits_l = W @ embd\n",
    "    closest_inds = torch.argsort(logits_l, descending=True)[:5]\n",
    "    for i, ind in enumerate(closest_inds):\n",
    "        most_probable_token = processor.decode(ind)\n",
    "        print(f'rank {i} hidden state is closest to: {most_probable_token}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store hidden states of the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ddl_inference = defaultdict(list)\n",
    "for _, row in tqdm(df_data.iterrows(), total=len(df_data)):\n",
    "    image = Image.open(row.image_path)\n",
    "    question = 'Describe the animal in the image?'\n",
    "    prompt = f\"[INST] <image>\\n{question} [/INST]\"\n",
    "    inputs = processor(prompt, image, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**inputs, max_new_tokens=100, output_hidden_states=True, return_dict_in_generate=True)\n",
    "    # result_text = processor.decode(output['sequences'][0], skip_special_tokens=False)\n",
    "    # result_text = result_text.split(prompt)[-1].strip()\n",
    "    hidden_states = output['hidden_states']\n",
    "    hidden_states = tuple(tuple(t.detach().cpu().numpy() for t in inner_tuple) for inner_tuple in hidden_states)\n",
    "    input_ids = list(inputs['input_ids'][0].detach().cpu().numpy())      # [1, 733, 16289\n",
    "    output_ids = list(output['sequences'][0].detach().cpu().numpy())     # [1, 733, 16289, ...\n",
    "\n",
    "    ddl_inference['hidden_states'].append(hidden_states)\n",
    "    ddl_inference['input_ids'].append(input_ids)\n",
    "    ddl_inference['output_ids'].append(output_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifier on different hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(ids: List[int]):\n",
    "    return [processor.tokenizer.decode(i) for i in ids]\n",
    "\n",
    "\n",
    "def get_text(ids: List[int]):\n",
    "    return processor.decode(ids)  \n",
    "\n",
    "\n",
    "def print_data(input_ids, output_ids, hidden_states):\n",
    "    input_ids                               # [1, 733, 16289\n",
    "    input_tokens = get_tokens(input_ids)    # ['<s>', '[', 'INST', ..\n",
    "    input_text = get_text(input_ids)        # '<s> [INST...\n",
    "    \n",
    "    output_ids                              # [1, 733, 16289, ...\n",
    "    output_tokens = get_tokens(output_ids)  # ['<s>', '[', 'INST', ..\n",
    "    output_text = get_text(output_ids)      # '<s> [INST...\n",
    "    \n",
    "    assert input_ids == output_ids[:len(input_ids)]\n",
    "    assert input_tokens == output_tokens[:len(input_tokens)]\n",
    "    assert input_text == output_text[:len(input_text)]\n",
    "    assert len(hidden_states) == len(output_ids) - len(input_ids)\n",
    "    \n",
    "    hs = hidden_states[time][layer][sample][location]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "    print(hs.shape)\n",
    "    # hs = hs.detach().cpu()\n",
    "    logits_l = W @ hs\n",
    "    closest_inds = torch.argsort(logits_l, descending=True)[:1]\n",
    "    for i, ind in enumerate(closest_inds):\n",
    "        most_probable_token = processor.decode(ind)\n",
    "        print(f'rank {i} hidden state is closest to: {most_probable_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "class_names = sorted(list(df_data.class_name.unique()))\n",
    "labels = df_data.class_name.apply(lambda name: class_names.index(name)).to_list()\n",
    "y = labels\n",
    "\n",
    "\n",
    "# define grid\n",
    "token_locs = list(range(-len(input_ids), 0))\n",
    "token_locs = [-3]\n",
    "layers = list(range(n_layers))\n",
    "layers = list(range(n_layers-4, n_layers))\n",
    "\n",
    "pbar = tqdm(total=len(token_locs) * len(layers))\n",
    "\n",
    "time = 0\n",
    "sample = 0\n",
    "\n",
    "ddl = defaultdict(list)\n",
    "for token_loc in token_locs:\n",
    "    for layer in layers:\n",
    "        X = np.array([hs[time][layer][sample][token_loc] for hs in ddl_inference['hidden_states']])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        classifier = MLPClassifier(max_iter=200)\n",
    "        \n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "    \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "\n",
    "        ddl['token_loc'].append(token_loc)\n",
    "        ddl['layer'].append(layer)\n",
    "        ddl['accuracy'].append(accuracy)\n",
    "        pbar.update(1)\n",
    "    \n",
    "        # print(\"Accuracy:\", accuracy)\n",
    "        # print(\"Classification Report:\\n\", report)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = pd.DataFrame(ddl)\n",
    "df_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./training_results.pickle', 'wb') as f:\n",
    "#     pickle.dump(ddl, f)\n",
    "\n",
    "with open('./training_results.pickle', 'rb') as f:\n",
    "    ddl_loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = pd.DataFrame(ddl_loaded)\n",
    "df_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_locs = df_acc.token_loc.unique()\n",
    "token_ids = np.array(input_ids)[token_locs]\n",
    "tokens = processor.tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(25, 8))\n",
    "heatmap_data = df_acc.pivot(columns=[\"token_loc\"], index=\"layer\", values=\"accuracy\")\n",
    "ax = sns.heatmap(heatmap_data, annot=True, cmap=\"YlGnBu\", )\n",
    "ax.invert_yaxis()\n",
    "plt.title('Accuracy Heatmap')\n",
    "plt.xticks(np.array(range(len(tokens)))+0.5, tokens)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Answer the question using a single word or a phrase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_acc[df_acc.token_loc == -5]\n",
    "x = df_temp.layer\n",
    "y = df_temp.accuracy\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title('Classifier Accuracy v.s. Hidden Layer Depth')\n",
    "plt.grid(True)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('layer')\n",
    "plt.plot(x, y, label='trained classifeir accuracy')\n",
    "plt.axhline(1 / len(class_names), label='random guess', c='red')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
