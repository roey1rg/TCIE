{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8f706-5ba9-4c3e-a471-ed04225688ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import nlp_utils\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from quintuplets import *\n",
    "from nlp_utils import add_text_to_image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64ffa7-35ea-460c-8b3e-1c705c059e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = nlp_utils.get_cache_dir()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6997ac-cdcb-4791-8d04-4ec363dc47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/dcor/roeyron/datasets/quintuplets_v1\"\n",
    "qp_ids = np.random.RandomState(1).permutation(sorted(os.listdir(dataset_path)))\n",
    "qp_id = qp_ids[0]\n",
    "qp = Quintuplet.load(dataset_path, qp_id)\n",
    "print(qp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af335271-a011-48b2-8a0a-b351ed23496c",
   "metadata": {},
   "source": [
    "# load Llava and clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a075cae2-d01b-4d25-9c2f-80c4bc12b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float16, cache_dir=cache_dir)\n",
    "model.generation_config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee60e1-77b5-4915-a974-7d0ba79c3818",
   "metadata": {},
   "source": [
    "# get Llava's text description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc65b7c2-7aa0-4e51-b3ea-0d42604891bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llava_desc(image, text):\n",
    "    prompt = f\"[INST] <image>\\n{text} \\n Limit your response to no more than 2 short sentences. [/INST]\"\n",
    "    #prompt = f\"[INST] <image>\\n{text} \\n Answer shortly, with few words only. [/INST]\"\n",
    "    inputs = processor(prompt, image, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**inputs, max_new_tokens=100, output_hidden_states=True, return_dict_in_generate=True)\n",
    "    result_text = processor.decode(output['sequences'][0], skip_special_tokens=True)\n",
    "    result_text = result_text.split(prompt.replace('<image>', ' '))[1].strip()\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5fc343-415b-421d-a1cd-88f9d96ec4da",
   "metadata": {},
   "source": [
    "# get CLIP similarity of an image and a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e056b77-ab38-48ba-b0b7-8c52aa7c9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_image_text_similarity(image, text):\n",
    "    image_processed = clip_preprocess(image).unsqueeze(0).to(device)\n",
    "    text_tokenized = clip.tokenize([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image_processed)\n",
    "        text_features = clip_model.encode_text(text_tokenized)\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "    sim = cos(image_features[0],text_features[0]).item()\n",
    "    sim = (sim+1)/2\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3094b3e1-c108-4ef4-a80a-cdb4b3c494cb",
   "metadata": {},
   "source": [
    "# Evaluate Quadruplets Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002e59f4-0a78-4dcc-a508-7b6db532c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0\n",
    "qn_ids = get_splits_ids()['train']\n",
    "qd_ids = [QuadrupletId(qn_id, which) for qn_id in qn_ids for which in [\"gamma_positive\", \"delta_positive\"]]\n",
    "for qd_id in qd_ids:\n",
    "    qd = load_quadruplet(QUINTUPLETS_DATASET_PATH, qd_id)\n",
    "    desc = get_llava_desc(qd.query, qd.prompt)\n",
    "    pos_score = get_clip_image_text_similarity(qd.positive, desc)\n",
    "    neg_score = get_clip_image_text_similarity(qd.negative, desc)\n",
    "    if pos_score > neg_score:\n",
    "        score += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f980b-3396-4b82-bcb6-d07b40c2f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "score / len(qd_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
